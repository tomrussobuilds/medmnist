{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Orchard ML: Optuna Model Search on Galaxy10 (GPU)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tomrussobuilds/orchard-ml/blob/main/notebooks/02_galaxy10_optuna_model_search.ipynb)\n\n> **Runtime**: This notebook requires a **GPU runtime**. In Colab: `Runtime > Change runtime type > T4 GPU`\n\nThis notebook demonstrates Orchard ML's **automatic hyperparameter optimization with model search** using Optuna:\n\n- **Dataset**: [Galaxy10 DECals](https://zenodo.org/records/10845026) (224x224 RGB, 10 galaxy morphology classes)\n- **Model search**: Optuna explores EfficientNet-B0, ConvNeXt-Tiny, ViT-Tiny, and ResNet-18 automatically\n- **Time**: ~30-45 minutes on Colab T4 GPU\n\n### What you'll learn\n1. How to configure Optuna hyperparameter search with `enable_model_search: true`\n2. How Orchard ML runs optimization trials, then trains with the best config\n3. How to interpret optimization results (parameter importances, trial history)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n%cd /content\nif not os.path.isdir(\"orchard-ml\"):\n    !git clone --depth 1 https://github.com/tomrussobuilds/orchard-ml.git\n\n%cd /content/orchard-ml\n!git pull --ff-only\n%pip install -q -r requirements.txt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"GPU not detected. This notebook requires a GPU runtime.\\n\"\n",
    "        \"In Colab: Runtime > Change runtime type > T4 GPU > Save\\n\"\n",
    "        \"If already set to T4, Colab may be out of GPU quota — try again later.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "We create a custom YAML config that enables **model search** — Optuna will explore different architectures\n",
    "(EfficientNet-B0, ViT-Tiny, ConvNeXt-Tiny, ResNet-18) alongside hyperparameters like learning rate, dropout, and augmentation.\n",
    "\n",
    "Key settings for Colab:\n",
    "- `n_trials: 5` — enough to explore the search space without exceeding runtime limits\n",
    "- `epochs: 5` — short optimization trials (final training uses full 20 epochs)\n",
    "- `enable_model_search: true` — lets Optuna pick the best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile colab_galaxy10_search.yaml\n",
    "# Galaxy10 Model Search — Colab-optimized config\n",
    "\n",
    "dataset:\n",
    "  name: \"galaxy10\"\n",
    "  data_root: ./dataset\n",
    "  resolution: 224\n",
    "  force_rgb: true\n",
    "  use_weighted_sampler: false\n",
    "\n",
    "architecture:\n",
    "  name: \"efficientnet_b0\"        # Default; Optuna will override with model search\n",
    "  pretrained: true\n",
    "  dropout: 0.3\n",
    "\n",
    "training:\n",
    "  seed: 42\n",
    "  batch_size: 16\n",
    "  learning_rate: 0.0001\n",
    "  weight_decay: 0.0005\n",
    "  momentum: 0.9\n",
    "  min_lr: 1e-7\n",
    "  mixup_alpha: 0.0\n",
    "  label_smoothing: 0.0\n",
    "  epochs: 20                      # Final training after optimization\n",
    "  patience: 10\n",
    "  grad_clip: 1.0\n",
    "  mixup_epochs: 0\n",
    "  scheduler_type: \"cosine\"\n",
    "  cosine_fraction: 0.5\n",
    "  scheduler_patience: 5\n",
    "  scheduler_factor: 0.1\n",
    "  step_size: 20\n",
    "  use_amp: true\n",
    "  use_tta: false\n",
    "  criterion_type: \"cross_entropy\"\n",
    "  weighted_loss: false\n",
    "  focal_gamma: 2.0\n",
    "\n",
    "augmentation:\n",
    "  hflip: 0.3\n",
    "  rotation_angle: 3\n",
    "  jitter_val: 0.05\n",
    "  min_scale: 0.97\n",
    "  tta_translate: 0.5\n",
    "  tta_scale: 1.02\n",
    "  tta_blur_sigma: 0.1\n",
    "\n",
    "hardware:\n",
    "  device: \"auto\"\n",
    "\n",
    "telemetry:\n",
    "  output_dir: ./outputs\n",
    "  log_level: \"INFO\"\n",
    "  log_interval: 50\n",
    "\n",
    "evaluation:\n",
    "  batch_size: 32\n",
    "  n_samples: 12\n",
    "  fig_dpi: 150\n",
    "  cmap_confusion: Blues\n",
    "  plot_style: seaborn-v0_8-muted\n",
    "  grid_cols: 4\n",
    "  fig_size_predictions: [12, 8]\n",
    "  report_format: xlsx\n",
    "  save_confusion_matrix: true\n",
    "  save_predictions_grid: true\n",
    "\n",
    "tracking:\n",
    "  enabled: false\n",
    "\n",
    "optuna:\n",
    "  study_name: \"galaxy10_model_search_colab\"\n",
    "  n_trials: 5\n",
    "  epochs: 5                       # Short trials for speed\n",
    "  timeout: null\n",
    "  metric_name: \"auc\"\n",
    "  direction: \"maximize\"\n",
    "  enable_early_stopping: true\n",
    "  early_stopping_threshold: 0.9999\n",
    "  early_stopping_patience: 2\n",
    "  sampler_type: \"tpe\"\n",
    "  search_space_preset: \"full\"\n",
    "  enable_model_search: true       # Explore architectures automatically\n",
    "  enable_pruning: true\n",
    "  pruner_type: \"median\"\n",
    "  pruning_warmup_epochs: 3\n",
    "  storage_type: \"sqlite\"\n",
    "  storage_path: null\n",
    "  n_jobs: 1\n",
    "  load_if_exists: true\n",
    "  show_progress_bar: false\n",
    "  save_plots: true\n",
    "  save_best_config: true\n",
    "\n",
    "export:\n",
    "  format: onnx\n",
    "  opset_version: 18\n",
    "  validate_export: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Run Optimization + Training\n\nOrchard ML automatically executes the full pipeline:\n1. **Optimization** — 5 Optuna trials, each training for 5 epochs. Optuna explores different architectures and hyperparameters.\n2. **Training** — Full 20-epoch training using the best configuration found.\n3. **Export** — ONNX model export for deployment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python forge.py --config colab_galaxy10_search.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Results\n",
    "\n",
    "### 4.1 Optimization artifacts\n",
    "\n",
    "Optuna generates interactive HTML plots and a `best_config.yaml` with the winning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Find the latest run directory\n",
    "run_dirs = sorted(glob.glob(\"outputs/*/\"))\n",
    "latest_run = run_dirs[-1]\n",
    "print(f\"Run directory: {latest_run}\\n\")\n",
    "\n",
    "# List all generated artifacts\n",
    "for root, dirs, files in os.walk(latest_run):\n",
    "    level = root.replace(latest_run, \"\").count(os.sep)\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = \"  \" * (level + 1)\n",
    "    for file in sorted(files):\n",
    "        size = os.path.getsize(os.path.join(root, file))\n",
    "        print(f\"{sub_indent}{file} ({size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\n\n# Show the best config found by Optuna\nbest_configs = glob.glob(f\"{latest_run}/reports/best_config*.yaml\")\nif best_configs:\n    with open(best_configs[0]) as f:\n        best_cfg = yaml.safe_load(f)\n    print(\"Best configuration found by Optuna:\")\n    print(yaml.dump(best_cfg, default_flow_style=False, sort_keys=False))\nelse:\n    print(\"best_config.yaml not found (check outputs directory)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Optuna visualizations\n\nOrchard ML generates interactive Plotly plots showing parameter importance and optimization history."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display parameter importance plot\n",
    "importance_files = glob.glob(f\"{latest_run}/figures/param_importances*.html\")\n",
    "if importance_files:\n",
    "    print(\"Parameter Importances (which hyperparameters matter most):\")\n",
    "    with open(importance_files[0]) as f:\n",
    "        display(HTML(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display optimization history\n",
    "history_files = glob.glob(f\"{latest_run}/figures/optimization_history*.html\")\n",
    "if history_files:\n",
    "    print(\"Optimization History (AUC across trials):\")\n",
    "    with open(history_files[0]) as f:\n",
    "        display(HTML(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 Final training results\n\nAfter optimization, Orchard ML trains the best model for the full 20 epochs and generates evaluation artifacts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# Display confusion matrix\n",
    "cm_files = glob.glob(f\"{latest_run}/figures/confusion_matrix*.png\")\n",
    "if cm_files:\n",
    "    print(\"Confusion Matrix (final model):\")\n",
    "    display(Image(filename=cm_files[0], width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display predictions grid\n",
    "pred_files = glob.glob(\n",
    "    f\"{latest_run}/figures/sample_predictions*.png\"\n",
    ")\n",
    "if pred_files:\n",
    "    print(\"Sample Predictions (Galaxy morphology):\")\n",
    "    display(Image(filename=pred_files[0], width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Next Steps\n\n- **Scale up**: Increase `n_trials` to 20 and `epochs` to 15 for more thorough optimization (see `recipes/optuna_galaxy10_efficientnet_b0.yaml`)\n- **CPU-friendly demo**: See [01_quickstart_bloodmnist_cpu.ipynb](./01_quickstart_bloodmnist_cpu.ipynb) for a quick intro without GPU\n- **MedMNIST datasets**: Swap `galaxy10` with any of the MedMNIST datasets (e.g., `pathmnist`, `dermamnist`, `octmnist`)\n- **Documentation**: Check the [Optimization Guide](https://github.com/tomrussobuilds/orchard-ml/blob/main/docs/guide/OPTIMIZATION.md) for advanced search space configuration"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}